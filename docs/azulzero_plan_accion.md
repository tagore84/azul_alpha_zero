# ğŸ§  Plan de AcciÃ³n para Mejorar el Entrenamiento de AzulZero  
### _(Ordenado por Prioridad y Optimizado para AplicaciÃ³n Directa)_

---

## 1ï¸âƒ£ Reducir el nÃºmero mÃ¡ximo de rondas (CRÃTICO)
- Ajustar **max_rounds = 6 o 7**.  
- Evita que el modelo aprenda estrategias degeneradas basadas en â€œaguantarâ€.  
- Fuerza que el agente **complete filas**, **cierre partidas** y **puntÃºe**, como en Azul real.

---

## 2ï¸âƒ£ Ajustar `temp_threshold` para permitir exploraciÃ³n real
- Establecer **temp_threshold = 8**.  
- Permite que los primeros movimientos tengan exploraciÃ³n por temperatura.  
- Evita la polÃ­tica determinista desde la ronda 1, que produce datasets degenerados.

---

## 3ï¸âƒ£ Incrementar `cpuct` para mejorar el balance entre exploraciÃ³n y explotaciÃ³n
- Ajustar **cpuct = 2.0**.  
- Con 200 simulaciones, este valor permite que MCTS explore mÃ¡s lÃ­neas Ãºtiles.  
- Corrige el comportamiento excesivamente conservador observado.

---

## 4ï¸âƒ£ Rebalancear la recompensa para enfatizar cierre y progreso
- Aumentar el peso de completar:
  - Filas  
  - Columnas  
  - Sets de colores  
- AÃ±adir una penalizaciÃ³n progresiva si no se completan filas tras varias rondas.  
- Esto enseÃ±a al modelo a **cerrar patrones**, no solo a evitar penalizaciones.

---

## 5ï¸âƒ£ Mantener o aumentar la penalizaciÃ³n por alcanzar el lÃ­mite de rondas
- Refuerza la idea de que terminar la partida pronto es lo Ã³ptimo.  
- Evita que el modelo busque â€œrondas extrasâ€ artificiales.

---

## 6ï¸âƒ£ Introducir ruido en el self-play (solo los primeros movimientos)
- **noise_eps = 0.35**  
- **dirichlet_alpha = 0.3**  
- Solo aplicarlo en los primeros 2â€“3 movimientos.  
- Esto evita que el self-play colapse en secuencias repetidas.

---

## 7ï¸âƒ£ Aumentar ligerÃ­simamente el nÃºmero de simulaciones (opcional)
- Subir a **simulations = 300** si el tiempo lo permite.  
- MCTS podrÃ¡ ver planes mÃ¡s profundos relacionados con cierre de filas y bonus.

---

## 8ï¸âƒ£ Verificar que el heurÃ­stico no tenga ventajas injustas
- Confirmar que no evalÃºa bonus finales de forma exacta.  
- Comprobar que no usa reglas del tipo:
  - â€œsi cojo X completo columna -> +7â€  
  - â€œsi dejo Y al rival, completa setâ€  
- Debe jugar bajo las mismas limitaciones que la red.

---

## 9ï¸âƒ£ Monitorizar si la red comienza a completar filas y provocar finales
- Esto debe empezar a verse entre **los ciclos 9 y 12**.  
- Si no aparece progreso:
  - Reajustar recompensas  
  - Revisar dataset  
  - Revisar polÃ­tica de exploraciÃ³n

---

## ğŸ”Ÿ Mantener dataset grande y evitar acumulaciÃ³n de partidas malas
- Conservar partidas recientes (Ãºltimos N ciclos).  
- **Eliminar o reducir** partidas donde:
  - se llega sistemÃ¡ticamente a max_rounds,  
  - ambos jugadores acaban con puntuaciones negativas,  
  - no se completan filas.  
- Esto elimina *ruido tÃ³xico* del entrenamiento.

---

## âœ” Resumen final del plan
Este conjunto de cambios transforma el modelo desde una polÃ­tica degenerada (â€œevitar puntos negativos durante 8 rondasâ€) hacia un estilo de juego genuinamente Ã³ptimo de Azul:

- cerrar filas,  
- prevenir columnas del rival,  
- forzar finales,  
- maximizar bonus,  
- jugar agresivo y tÃ¡ctico.

---

Â¿Quieres que tambiÃ©n genere una **versiÃ³n PDF**, **DOCX**, un **README tÃ©cnico** o un **diagrama visual del pipeline**?

