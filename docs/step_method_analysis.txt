ANÁLISIS DEL MÉTODO step() — DETECTANDO BUGS Y PROPONIENDO SOLUCIONES
=====================================================================

Este documento resume los problemas detectados en el método `step()` del entorno AzulZero, así como sus causas y correcciones recomendadas.

---------------------------------------------------------------------
1. BUG CRÍTICO: Se modifica el score REAL al penalizar max_rounds
---------------------------------------------------------------------

Código problemático:
    p['score'] -= 25
    reward = -25

Problemas:
- El score del juego ya no representa Azul real.
- Rompe heurísticos, logs, análisis y el estado interno.
- El reward y el score deben estar separados completamente.

Solución:
- Eliminar la modificación de score.
- Mantener solo reward = -25.
- El score del jugador debe reflejar únicamente reglas oficiales.

---------------------------------------------------------------------
2. BUG: La recompensa final solo considera la puntuación del jugador activo
---------------------------------------------------------------------

Código actual:
    reward = (p['score'] - before_score) - 0.5 * (self.players[opponent]['score'] - opponent_score_before)

Problemas:
- En Azul, cuando finaliza la partida, ambos jugadores reciben bonus.
- El reward no refleja el impacto del bonus del rival.
- Sesga el aprendizaje hacia jugadas que parecen positivas pero no lo son en el resultado global.

Solución:
    delta_self = p['score'] - before_score
    delta_opp  = self.players[opponent]['score'] - opponent_score_before
    reward = delta_self - delta_opp

---------------------------------------------------------------------
3. BUG DE LÓGICA: La rotación de turnos es incorrecta tras finalizar una ronda
---------------------------------------------------------------------

Código actual:
    if self._is_round_over():
        done = self._end_round()
        ...
    else:
        self.current_player = opponent

Problema:
- Cuando una ronda termina pero la partida continúa, el turno NO DEBERÍA cambiar automáticamente al jugador contrario.
- Según Azul, la siguiente ronda debe comenzar quien tomó el token (-1).

Impacto:
- Jugadores repiten turnos incorrectamente.
- Desalineación con reglas oficiales.
- MCTS aprende partidas ilegales.

Solución:
- El método `_end_round()` debe controlar completamente quién empieza la siguiente ronda.
- El `step()` no debe asignar current_player en caso de fin de ronda.

---------------------------------------------------------------------
4. BUG: Overflow en floor_line ignora límites
---------------------------------------------------------------------

Código:
    for _ in range(overflow):
        idxs = np.where(fl == -1)[0]
        if idxs.size > 0:
            fl[idxs[0]] = color

Problemas:
- Si el floor está lleno, las fichas extra DESAPARECEN en vez de ir al descarte.
- Penalizaciones incorrectas.
- Distorsión en el aprendizaje del MCTS.

Solución:
- Cuando el floor esté lleno, enviar todo exceso al descarte.
- La penalización debe aplicarse correctamente.

---------------------------------------------------------------------
5. PROBLEMA DE DISEÑO: Reward demasiado dependiente de score inmediato
---------------------------------------------------------------------

Efectos:
- La red aprende comportamiento defensivo.
- No aprende a completar filas, terminar partida o bloquear al rival.
- Estrategias degeneradas que "evitan perder" pero "no buscan ganar".

Solución recomendada:
- Reward = Δ propia - Δ rival
- Penalización fuerte por max_rounds
- Bonus por completar filas/columnas/sets
- Penalización por llenar suelo repetidamente

---------------------------------------------------------------------
6. CONCLUSIÓN GENERAL
---------------------------------------------------------------------

El método step() funciona pero contiene:
- 3 bugs lógicos graves
- 1 bug de overflow
- 1 diseño de reward subóptimo

Estos problemas explican:
- partidas excesivamente largas,
- puntuaciones bajas,
- políticas degeneradas,
- 0 victorias contra el heurístico.

Prioridades de corrección:
1) No tocar score real / corregir reward final
2) Arreglar turnos post-ronda
3) Arreglar overflow floor_line
4) Rediseñar reward shaping con enfoque AlphaZero

