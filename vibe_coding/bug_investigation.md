# Investigaci贸n de Bugs en Entrenamiento Azul Zero

Este documento detalla los posibles puntos de fallo identificados tras el an谩lisis de la regresi贸n en el Ciclo 2 y revisi贸n del c贸digo.

##  Hallazgo Cr铆tico: Ceguera Temporal (Missing Feature)

**Archivo**: `src/azul/env.py`
**Funci贸n**: `encode_observation`

El agente **no recibe informaci贸n sobre la ronda actual**, a pesar de que la documentaci贸n de la funci贸n dice que s铆.

- **Evidencia**: En `src/azul/env.py`, la funci贸n `encode_observation` construye el vector `global_parts` con `bag`, `discard`, `scores`, etc., pero **omite expl铆citamente `round_count`**.
- **Consecuencia**: El agente no tiene "reloj". No puede distinguir entre la Ronda 1 y la Ronda 14.
- **Impacto en el Problema Observable**: El entrenamiento penaliza severamente las partidas que llegan a `MaxRounds` (asignando -1.0 a ambos jugadores). Sin embargo, como el agente es "ciego al tiempo", no puede aprender a acelerar o tomar riesgos calculados cuando se acerca el final. Percibe la penalizaci贸n de tiempo como ruido aleatorio, lo que lleva a un comportamiento err谩tico o de "zombi" (jugar pasivamente).

## 锔 Problema de Estabilidad: Acantilado de Recompensa (Reward Cliff)

**Archivo**: `src/train/self_play.py`
**L贸gica**: `max_rounds_reached` Override

Cuando una partida alcanza el l铆mite de rondas, el sistema anula el resultado y asigna `v = -1.0` (derrota total) a **ambos** jugadores.

- **Discontinuidad**:
    - Ronda 14: Jugador A gana por puntos (-150 vs -160). Recompensa: **+0.5** (aprox).
    - Ronda 15 (L铆mite): Jugador A tiene los mismos puntos. Recompensa: **-1.0** (Override).
- **Conflicto**: Combinado con la "Ceguera Temporal", el agente ve que una estrategia ganadora se convierte repentinamente en una derrota catastr贸fica sin ninguna se帽al de aviso en el estado. Esto dificulta enormemente la convergencia.

## 癸 Observaciones Menores

1.  **Redundancia en Limpieza de Floor Line**:
    - En `env.py`, m茅todo `_end_round`, se hace `p['floor_line'][p['floor_line'] == 5] = -1` justo antes de hacer `p['floor_line'][:] = -1`. La primera l铆nea es redundante. No es un bug funcional, pero ensucia el c贸digo.

## Plan de Acci贸n Propuesto

1.  **Arreglar Ceguera Temporal**: A帽adir `round_count` (normalizado, ej. `round / max_rounds`) al vector de observaci贸n en `env.py`.
2.  **Suavizar Penalizaci贸n por Tiempo**:
    - En lugar de anular el score con -1.0, aplicar una penalizaci贸n fuerte pero aditiva al score final (ej. `score - 50`), o;
    - Mantener el override pero asegurar que el agente tenga el input de `round_count` para poder predecirlo. *Recomiendo primero arreglar el input y ver si el agente aprende a evitar el timeout.*

Este documento sirve como base para aplicar correcciones en la siguiente fase.
